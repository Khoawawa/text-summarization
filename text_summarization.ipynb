{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSTALL DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install evaluate\n",
    "!pip install numpy as np\n",
    "!pip install request\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD BART-LARGE-CNN MODEL AND CNN_DAILYMAIL TEST DATASET FOR EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "bart_pipe = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds_test = load_dataset(\"abisee/cnn_dailymail\",\"3.0.0\", split = \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATE BART-LARGE-CNN MODEL USING BERTSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_eval_articles = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_text(text, chunk_size):\n",
    "    chunks = []\n",
    "    for i in range(0,len(text), chunk_size):\n",
    "        chunk = text[i:i+chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(bart_pipe, text, chunk_size):\n",
    "    chunks = chunked_text(text,chunk_size)\n",
    "    \n",
    "    summaries = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        summary = bart_pipe(chunk,max_length = 130, min_length = 1, do_sample = False)[0]['summary_text']\n",
    "        \n",
    "        summaries.append(summary)\n",
    "        \n",
    "    return ' '.join(summaries)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_summaries = []\n",
    "ref_summaries = []\n",
    "CHUNK_SIZE = 1024\n",
    "for i in range(no_eval_articles):\n",
    "    article = ds_test[i]['article']\n",
    "    summary = ds_test[i]['highlights']\n",
    "    # SUMMARIZE\n",
    "    bart_summary = summarize(bart_pipe,article,CHUNK_SIZE)\n",
    "    \n",
    "    bart_summaries.append(article, bart_summary)\n",
    "    ref_summaries.append(article, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bert_score = load(\"bertscore\")\n",
    "\n",
    "results = bert_score.compute(predictions=bart_summaries, references=ref_summaries, model_type=\"facebook/bart-large-cnn\")\n",
    "\n",
    "f1s = results['f1']\n",
    "precisions = results['precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1: {np.average(f1s,4)}\")\n",
    "print(f\"Precisions: {np.average(precisions,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET DATA FROM CNN WEBSITE AND SUMMARIZE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_cnn_article(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # For CNN articles\n",
    "        if \"cnn.com\" in url:\n",
    "            title = soup.find('h1').get_text()\n",
    "            article_body = soup.find_all('p', class_=\"paragraph inline-placeholder vossi-paragraph\")\n",
    "            content = \" \".join([p.get_text() for p in article_body])       \n",
    "            return title, content\n",
    "        else:\n",
    "            return None,None\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve the article. Status code: {response.status_code}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title, content = scrape_cnn_article(\"https://edition.cnn.com/2024/10/20/politics/mcdonalds-donald-trump-pennsylvania/index.html\")\n",
    "\n",
    "data ={\n",
    "    'title': title,\n",
    "    'article': content,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_summary = summarize(bart_pipe,data['article'],CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
